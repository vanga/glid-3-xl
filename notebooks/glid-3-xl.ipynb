{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b224e37d",
   "metadata": {},
   "source": [
    "This notebook is a modified version of [kaggle notebook](kaggle.com/code/litevex/lite-s-latent-diffusion-v9-with-gradio), in an attempt to make it simpler and configurable.\n",
    "**Known issues**\n",
    "* selecting clip_variant other than the default + `jack` configuration leads to an error\n",
    "* ViT-L/14 may cause memory issues on GPUs other than A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!sudo apt -y -qq install imagemagick \n",
    "!pip install -qq timm\n",
    "!pip install -q gradio\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68987f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $base_dir\n",
    "!git clone -qq https://github.com/CompVis/latent-diffusion\n",
    "!git clone -qq https://github.com/CompVis/taming-transformers\n",
    "\n",
    "!pip install -e -qq ./taming-transformers\n",
    "\n",
    "%cd $base_dir/latent-diffusion\n",
    "!git clone -qq https://github.com/Lin-Sinorodin/SwinIR_wrapper.git\n",
    "!git clone https://github.com/Jack000/glid-3-xl\n",
    "!pip install -qq -e ./glid-3-xl\n",
    "\n",
    "!pip install -qq git+https://github.com/lucidrains/DALLE-pytorch\n",
    "\n",
    "\n",
    "!mkdir -p $base_dir/working\n",
    "!wget https://cdn.discordapp.com/attachments/932425906847359016/968184632841486336/lite.css -O $base_dir/working/lite.css\n",
    "\n",
    "print(\"Restarting runtime, continue running next cells afterwards\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/ubuntu/vangap/glid-3-xl\"\n",
    "model_base_dir = f\"{base_dir}/checkpoints\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "finetune_path = f\"{model_base_dir}/finetune.pt\"\n",
    "base_path = f\"{model_base_dir}/diffusion.pt\"\n",
    "ldm_first_stage_path = f\"{model_base_dir}/kl-f8.pt\"\n",
    "inpaint_path = f\"{model_base_dir}/inpaint.pt\"\n",
    "bert_path = f\"{model_base_dir}/bert.pt\"\n",
    "\n",
    "clip_variant = 'ViT-L/14'\n",
    "\n",
    "# Some options that need to be set BEFORE pressing Run All (run > restart to change later on)\n",
    "# Models:\n",
    "'''\n",
    "\"jack\": This is the base model finetuned on a clean dataset of photographs by Jack000. It will produce better, higher resolution realistic images without watermarks,\n",
    "but might not be as good at flat illustrations, some prompts and writing text\n",
    "\n",
    "\"base\": This is the base 1.6B model released by CompVis trained on LAION-400M. It is better at illustrations but will sometimes produce blurry and watermarked images,\n",
    "write text even if unwanted and follow the prompt less.\n",
    "\n",
    "\"inpaint\": This is an inpainting model trained by jack0. If you use this, you have to set a mask image and use the Kaggle GUI.\n",
    "The mask should be the image size and black for spots to fill in, and white for areas to keep. (also try to avoid antialiasing)\n",
    "'''\n",
    "which_model = \"inpaint\" # jack, base, inpaint\n",
    "\n",
    "# GUIs:\n",
    "'''\n",
    "Kaggle: GUI using Jupyter Forms. It will show up in the notebook and have a small progress preview if you're generating a single image, but the layout is simpler,\n",
    "there's no API or queue and you can't share it with others\n",
    "\n",
    "Gradio: [Does not support the inpaint model] GUI using Gradio. It will give you a gradio.app link (as well as embed in the notebook) with a better layout\n",
    "that you can share with others, as well as an inbuilt API, but there's no progress preview.\n",
    "'''\n",
    "which_gui = \"gradio\" # kaggle, gradio\n",
    "\n",
    "steps = 25 # How many steps diffusion should run for. Not much improvement above 25, lower values might lose detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411555ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $base_dir/checkpoints/\n",
    "%cd $base_dir/checkpoints/\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/bert.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/kl-f8.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/diffusion.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/finetune.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/inpaint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889ec7e",
   "metadata": {},
   "source": [
    "%cd $base_dir/latent-diffusion/\n",
    "from SwinIR_wrapper.SwinIR_wrapper import SwinIR_SR\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#@title Setup Super Resolution Model { run: \"auto\" }\n",
    "pretrained_model = \"real_sr x4\" #@param [\"real_sr x4\", \"classical_sr x2\", \"classical_sr x3\", \"classical_sr x4\", \"classical_sr x8\", \"lightweight x2\", \"lightweight x3\", \"lightweight x4\"]\n",
    "\n",
    "model_type, scale = pretrained_model.split(' ')\n",
    "scale = int(scale[1])\n",
    "\n",
    "# initialize super resolution model\n",
    "sr = SwinIR_SR(model_type, scale)\n",
    "\n",
    "print(f'Loaded {pretrained_model} successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef730",
   "metadata": {},
   "source": [
    "#### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a224b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "\n",
    "sys.path.append(f\"{base_dir}/latent-diffusion/glid-3-xl\")\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "\n",
    "from dalle_pytorch import DiscreteVAE, VQGanVAE\n",
    "\n",
    "from einops import rearrange\n",
    "from math import log2, sqrt\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "from encoders.modules import BERTEmbedder\n",
    "\n",
    "import clip\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        if which_model == \"jack\":\n",
    "            self.model_path = finetune_path\n",
    "        elif which_model == \"base\":\n",
    "            self.model_path = base_path\n",
    "        else:\n",
    "            self.model_path = inpaint_path\n",
    "        self.kl_path = ldm_first_stage_path\n",
    "        self.bert_path = bert_path\n",
    "        self.text = ''\n",
    "        self.edit = ''\n",
    "        self.edit_x = 0\n",
    "        self.edit_y = 0\n",
    "        self.edit_width = 256\n",
    "        self.edit_height = 256\n",
    "        self.mask = ''\n",
    "        self.negative = ''\n",
    "        self.init_image = None\n",
    "        self.skip_timesteps = 0\n",
    "        self.prefix = ''\n",
    "        self.num_batches = 1\n",
    "        self.batch_size = 1\n",
    "        self.width = 256\n",
    "        self.height = 256\n",
    "        self.seed = -1\n",
    "        self.guidance_scale = 5.0\n",
    "        self.steps = 25\n",
    "        self.cpu = False\n",
    "        self.clip_score = False\n",
    "        self.clip_guidance = False\n",
    "        self.clip_guidance_scale = 150\n",
    "        self.cutn = 16\n",
    "        self.ddim = False\n",
    "        self.ddpm = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "print('Using device:', device)\n",
    "\n",
    "model_state_dict = torch.load(args.model_path, map_location='cpu')\n",
    "\n",
    "model_params = {\n",
    "    'attention_resolutions': '32,16,8',\n",
    "    'class_cond': False,\n",
    "    'diffusion_steps': 1000,\n",
    "    'rescale_timesteps': True,\n",
    "    'timestep_respacing': '27',  # Modify this value to decrease the number of\n",
    "                                 # timesteps.\n",
    "    'image_size': 32,\n",
    "    'learn_sigma': False,\n",
    "    'noise_schedule': 'linear',\n",
    "    'num_channels': 320,\n",
    "    'num_heads': 8,\n",
    "    'num_res_blocks': 2,\n",
    "    'resblock_updown': False,\n",
    "    'use_fp16': False,\n",
    "    'use_scale_shift_norm': False,\n",
    "    'clip_embed_dim': 768 if 'clip_proj.weight' in model_state_dict else None,\n",
    "    'image_condition': True if model_state_dict['input_blocks.0.0.weight'].shape[1] == 8 else False,\n",
    "    'super_res_condition': True if 'external_block.0.0.weight' in model_state_dict else False,\n",
    "}\n",
    "\n",
    "if args.ddpm:\n",
    "    model_params['timestep_respacing'] = 1000\n",
    "if args.ddim:\n",
    "    if args.steps:\n",
    "        model_params['timestep_respacing'] = 'ddim'+str(args.steps)\n",
    "    else:\n",
    "        model_params['timestep_respacing'] = 'ddim50'\n",
    "elif args.steps:\n",
    "    model_params['timestep_respacing'] = str(args.steps)\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config.update(model_params)\n",
    "\n",
    "if args.cpu:\n",
    "    model_config['use_fp16'] = False\n",
    "\n",
    "# Load models\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(model_state_dict, strict=False)\n",
    "model.requires_grad_(args.clip_guidance).eval().to(device)\n",
    "\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "else:\n",
    "    model.convert_to_fp32()\n",
    "\n",
    "def set_requires_grad(model, value):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = value\n",
    "\n",
    "# vae\n",
    "ldm = torch.load(args.kl_path, map_location=\"cpu\")\n",
    "ldm.to(device)\n",
    "ldm.eval()\n",
    "ldm.requires_grad_(args.clip_guidance)\n",
    "set_requires_grad(ldm, args.clip_guidance)\n",
    "\n",
    "bert = BERTEmbedder(1280, 32)\n",
    "sd = torch.load(args.bert_path, map_location=\"cpu\")\n",
    "bert.load_state_dict(sd)\n",
    "\n",
    "bert.to(device)\n",
    "bert.half().eval()\n",
    "set_requires_grad(bert, False)\n",
    "\n",
    "# clip\n",
    "clip_model, clip_preprocess = clip.load(clip_variant, device=device, jit=False)\n",
    "clip_model.eval().requires_grad_(False)\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5eebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import ImageFilter\n",
    "\n",
    "def do_run(ds, use_ds = True):\n",
    "    if args.seed >= 0:\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "    # bert context\n",
    "    text_emb = bert.encode([args.text]*args.batch_size).to(device).float()\n",
    "    text_blank = bert.encode([args.negative]*args.batch_size).to(device).float()\n",
    "\n",
    "    text = clip.tokenize([args.text]*args.batch_size, truncate=True).to(device)\n",
    "    text_clip_blank = clip.tokenize([args.negative]*args.batch_size, truncate=True).to(device)\n",
    "\n",
    "\n",
    "    # clip context\n",
    "    text_emb_clip = clip_model.encode_text(text)\n",
    "    text_emb_clip_blank = clip_model.encode_text(text_clip_blank)\n",
    "\n",
    "    make_cutouts = MakeCutouts(clip_model.visual.input_resolution, args.cutn)\n",
    "\n",
    "    text_emb_norm = text_emb_clip[0] / text_emb_clip[0].norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_embed = None\n",
    "\n",
    "    # image context\n",
    "    if args.edit:\n",
    "        if args.edit.endswith('.npy'):\n",
    "            print(\"ERROR: npy not supported (temp)\")\n",
    "        else:\n",
    "            w = args.edit_width if args.edit_width else args.width\n",
    "            h = args.edit_height if args.edit_height else args.height\n",
    "\n",
    "            input_image_pil = Image.open(fetch(args.edit)).convert('RGB')\n",
    "            input_image_pil = ImageOps.fit(input_image_pil, (w, h))\n",
    "\n",
    "            input_image = torch.zeros(1, 4, args.height//8, args.width//8, device=device)\n",
    "\n",
    "            im = transforms.ToTensor()(input_image_pil).unsqueeze(0).to(device)\n",
    "            im = 2*im-1\n",
    "            im = ldm.encode(im).sample()\n",
    "\n",
    "            y = args.edit_y//8\n",
    "            x = args.edit_x//8\n",
    "\n",
    "            input_image = torch.zeros(1, 4, args.height//8, args.width//8, device=device)\n",
    "\n",
    "            ycrop = y + im.shape[2] - input_image.shape[2]\n",
    "            xcrop = x + im.shape[3] - input_image.shape[3]\n",
    "\n",
    "            ycrop = ycrop if ycrop > 0 else 0\n",
    "            xcrop = xcrop if xcrop > 0 else 0\n",
    "\n",
    "            input_image[0,:,y if y >=0 else 0:y+im.shape[2],x if x >=0 else 0:x+im.shape[3]] = im[:,:,0 if y > 0 else -y:im.shape[2]-ycrop,0 if x > 0 else -x:im.shape[3]-xcrop]\n",
    "\n",
    "            input_image_pil = ldm.decode(input_image)\n",
    "            input_image_pil = TF.to_pil_image(input_image_pil.squeeze(0).add(1).div(2).clamp(0, 1))\n",
    "\n",
    "            input_image *= 0.18215\n",
    "\n",
    "        if args.mask:\n",
    "            mask_image = Image.open(fetch(args.mask)).convert('L')\n",
    "            mask_image = mask_image.resize((args.width//8,args.height//8), Image.ANTIALIAS)\n",
    "            mask = transforms.ToTensor()(mask_image).unsqueeze(0).to(device)\n",
    "\n",
    "        mask1 = (mask > 0.5)\n",
    "        mask1 = mask1.float()\n",
    "\n",
    "        input_image *= mask1\n",
    "\n",
    "        image_embed = torch.cat(args.batch_size*2*[input_image], dim=0).float()\n",
    "    elif model_params['image_condition']:\n",
    "        # using inpaint model but no image is provided\n",
    "        image_embed = torch.zeros(args.batch_size*2, 4, args.height//8, args.width//8, device=device)\n",
    "\n",
    "    kwargs = {\n",
    "        \"context\": torch.cat([text_emb, text_blank], dim=0).float(),\n",
    "        \"clip_embed\": torch.cat([text_emb_clip, text_emb_clip_blank], dim=0).float() if model_params['clip_embed_dim'] else None,\n",
    "        \"image_embed\": image_embed\n",
    "    }\n",
    "\n",
    "    # Create a classifier-free guidance sampling function\n",
    "    def model_fn(x_t, ts, **kwargs):\n",
    "        half = x_t[: len(x_t) // 2]\n",
    "        combined = torch.cat([half, half], dim=0)\n",
    "        model_out = model(combined, ts, **kwargs)\n",
    "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
    "        half_eps = uncond_eps + args.guidance_scale * (cond_eps - uncond_eps)\n",
    "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
    "        return torch.cat([eps, rest], dim=1)\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, context=None, clip_embed=None, image_embed=None):\n",
    "        with torch.enable_grad():\n",
    "            x = x[:args.batch_size].detach().requires_grad_()\n",
    "\n",
    "            n = x.shape[0]\n",
    "\n",
    "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "\n",
    "            kw = {\n",
    "                'context': context[:args.batch_size],\n",
    "                'clip_embed': clip_embed[:args.batch_size] if model_params['clip_embed_dim'] else None,\n",
    "                'image_embed': image_embed[:args.batch_size] if image_embed is not None else None\n",
    "            }\n",
    "\n",
    "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs=kw)\n",
    "\n",
    "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "\n",
    "            x_in /= 0.18215\n",
    "\n",
    "            x_img = ldm.decode(x_in)\n",
    "\n",
    "            clip_in = normalize(make_cutouts(x_img.add(1).div(2)))\n",
    "            clip_embeds = clip_model.encode_image(clip_in).float()\n",
    "            dists = spherical_dist_loss(clip_embeds.unsqueeze(1), text_emb_clip.unsqueeze(0))\n",
    "            dists = dists.view([args.cutn, n, -1])\n",
    "\n",
    "            losses = dists.sum(2).mean(0)\n",
    "\n",
    "            loss = losses.sum() * args.clip_guidance_scale\n",
    "\n",
    "            return -torch.autograd.grad(loss, x)[0]\n",
    " \n",
    "    if args.ddpm:\n",
    "        sample_fn = diffusion.ddpm_sample_loop_progressive\n",
    "    elif args.ddim:\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.plms_sample_loop_progressive\n",
    "\n",
    "    def save_sample(i, sample, clip_score=False, final = False):\n",
    "        for k, image in enumerate(sample['pred_xstart'][:args.batch_size]):\n",
    "            if args.batch_size == 1 or final:\n",
    "                image /= 0.18215\n",
    "                im = image.unsqueeze(0)\n",
    "                out = ldm.decode(im)\n",
    "            \n",
    "                out = TF.to_pil_image(out.squeeze(0).add(1).div(2).clamp(0, 1))\n",
    "                if use_ds and not final:\n",
    "                    # kaggle lags if you try to load a 256x256 image once a second so we pixelate it\n",
    "                    # also looks cooler\n",
    "                    out = out.resize((64, 64), Image.ANTIALIAS)\n",
    "                    out = out.resize((256, 256), Image.NEAREST)\n",
    "                filename = f'{base_dir}/output/{args.prefix}_progress_{i * args.batch_size + k:05}.png'\n",
    "                out.save(filename)\n",
    "                print(\"saved \" + filename)\n",
    "                if use_ds:\n",
    "                    if args.batch_size == 1:\n",
    "                        nImg = PImage(filename=filename)\n",
    "                        ds.update(nImg)\n",
    "                    else:\n",
    "                        ds.update(\"[no batch preview]\")\n",
    "\n",
    "    if args.init_image:\n",
    "        init = Image.open(args.init_image).convert('RGB')\n",
    "        init = init.resize((int(args.width),  int(args.height)), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).to(device).unsqueeze(0).clamp(0,1)\n",
    "        h = ldm.encode(init * 2 - 1).sample() *  0.18215\n",
    "        init = torch.cat(args.batch_size*2*[h], dim=0)\n",
    "    else:\n",
    "        init = None\n",
    "\n",
    "    for i in range(args.num_batches):\n",
    "        cur_t = diffusion.num_timesteps - 1\n",
    "\n",
    "        samples = sample_fn(\n",
    "            model_fn,\n",
    "            (args.batch_size*2, 4, int(args.height/8), int(args.width/8)),\n",
    "            clip_denoised=False,\n",
    "            model_kwargs=kwargs,\n",
    "            cond_fn=cond_fn if args.clip_guidance else None,\n",
    "            device=device,\n",
    "            progress=True,\n",
    "            init_image=init,\n",
    "            skip_timesteps=args.skip_timesteps,\n",
    "        )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            cur_t -= 1\n",
    "            if j % 5 == 0 and j != diffusion.num_timesteps - 1  and args.batch_size == 1 and use_ds:\n",
    "                save_sample(i, sample)\n",
    "\n",
    "        return save_sample(i, sample, args.clip_score, True)\n",
    "\n",
    "def swinUpscale(path, showLarger):\n",
    "    smallImg = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    hqImg = sr.upscale(smallImg)\n",
    "    # now downscale again, so it looks sharp\n",
    "    if showLarger == False:\n",
    "        resized_image = cv2.resize(hqImg, (0,0), fx=0.25, fy=0.25) \n",
    "    else:\n",
    "        resized_image = cv2.resize(hqImg, (0,0), fx=0.5, fy=0.5) \n",
    "    cv2.imwrite(path,resized_image)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de928c",
   "metadata": {},
   "source": [
    "#### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jupyter-widgets input[type=\"text\"]{\n",
    "    min-width: 90% !important;\n",
    "    border-color: #00000026;\n",
    "}\n",
    ".jupyter-widgets input[type=\"number\"]{\n",
    "    border-color: #00000026;\n",
    "}\n",
    ".jupyter-widgets input[type=\"text\"]:focus, .jupyter-widgets input[type=\"number\"]:focus{\n",
    "    border-bottom: 2px solid #0073ff7a;\n",
    "}\n",
    ".widget-button{\n",
    "    font-family: \"Segoe UI\";\n",
    "    background: #f0f0f0;\n",
    "    border-radius: 23px;\n",
    "    width: 130px;\n",
    "    height: 28px;\n",
    "    \n",
    "}\n",
    ".widget-button:hover{\n",
    "    box-shadow: none !important;\n",
    "    background: #d9d9d9;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "import time\n",
    "# from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Image as PImage\n",
    "# from IPython.display import display as PDisplay\n",
    "from os.path import exists\n",
    "import shutil\n",
    "import glob\n",
    "import gradio as gr\n",
    "class printer(str):\n",
    "    def __repr__(self):\n",
    "       return self\n",
    "\n",
    "def adv_run(prompt,negative,init_image,skips,guidance,batches,amount_per_batch,width,height,clip_rerank,swin_input,show_large):\n",
    "        args.text = prompt\n",
    "        args.negative = negative\n",
    "        if init_image != None:\n",
    "            args.init_image = init_image\n",
    "        else:\n",
    "            args.init_image = None\n",
    "        args.skip_timesteps = skips\n",
    "        args.guidance_scale = guidance\n",
    "        args.num_batches = batches\n",
    "        args.batch_size = amount_per_batch\n",
    "        args.width = width\n",
    "        args.height = height\n",
    "        args.clip_score = clip_rerank\n",
    "        shutil.rmtree(f'{base_dir}/output/', True)\n",
    "        os.makedirs(f\"{base_dir}/output/\", exist_ok=True)\n",
    "        win = do_run(None, False)\n",
    "        if args.batch_size > 1:\n",
    "            if swin_input == True:\n",
    "                for file in tqdm(glob.glob(f\"{base_dir}/output/*.png\")):\n",
    "                    swinUpscale(file,show_large)\n",
    "            !montage -geometry +1+1 -background black $base_dir/output/*.png $base_dir/grid.png\n",
    "            return Image.open(f\"{base_dir}/grid.png\")\n",
    "        if swin_input == True and args.batch_size == 1:\n",
    "            swinUpscale(f\"{base_dir}/output/_progress_00000.png\",show_large)\n",
    "            return Image.open(f\"{base_dir}/output/_progress_00000.png\")\n",
    "        if swin_input == False and args.batch_size == 1:\n",
    "            return Image.open(f\"{base_dir}/output/_progress_00000.png\")\n",
    "\n",
    "iface = gr.Interface(fn=adv_run, inputs=[\"text\",\"text\",gr.inputs.Image(shape=(256, 256), optional=True, type=\"filepath\"),gr.inputs.Slider(0, steps,1,default=0,label=\"Step Skips (required for init image)\"),gr.inputs.Slider(1, 15,1,default=5),gr.inputs.Slider(1, 32,1,default=1),gr.inputs.Slider(1, 16,1,default=1),gr.inputs.Slider(16, 512, 16,default=256),gr.inputs.Slider(16, 512, 16,default=256), gr.inputs.Checkbox(default=False, label=\"Clip Rerank (for batch images)\", optional=False),gr.inputs.Checkbox(default=True, label=\"Increase sharpness using SwinIR\", optional=False),gr.inputs.Checkbox(default=False, label=\"Show SwinIR results as 512x512 (less sharp)\", optional=False)\n",
    "], outputs=\"image\", css=f\"{base_dir}/lite.css\")\n",
    "iface.launch(share=True,debug=True, inline=False, enable_queue = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
