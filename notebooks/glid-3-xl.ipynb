{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[colab]: <https://colab.research.google.com/assets/colab-badge.svg>\n",
    "[colab-cin]: <https://colab.research.google.com/github/vanga/glid-3-xl/blob/master/notebooks/glid-3-xl.ipynb>\n",
    "\n",
    "[![][colab]][colab-cin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224e37d",
   "metadata": {},
   "source": [
    "This notebook is a modified version of [kaggle notebook](kaggle.com/code/litevex/lite-s-latent-diffusion-v9-with-gradio), in an attempt to make it simpler and configurable. Built on top of the [glid-3-xl fork](https://github.com/vanga/glid-3-xl)\n",
    "\n",
    "\n",
    "**Known issues**\n",
    "* selecting clip_model other than the default + `jack` configuration leads to a tensor size mismatch error (carried over from the original notebook)\n",
    "* ViT-L/14 may cause memory issues on GPUs other than A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
    "!sudo apt -y -qq install imagemagick \n",
    "!pip install -qq timm\n",
    "!pip install -q gradio\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68987f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/content/glid-3-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $base_dir\n",
    "!git clone -qq https://github.com/CompVis/latent-diffusion\n",
    "!git clone -qq https://github.com/CompVis/taming-transformers\n",
    "\n",
    "!pip install -e -qq ./taming-transformers\n",
    "\n",
    "%cd $base_dir/latent-diffusion\n",
    "!git clone -qq https://github.com/Lin-Sinorodin/SwinIR_wrapper.git\n",
    "!git clone https://github.com/vangap/glid-3-xl\n",
    "!pip install -qq -e ./glid-3-xl\n",
    "\n",
    "!pip install -qq git+https://github.com/lucidrains/DALLE-pytorch\n",
    "\n",
    "\n",
    "!mkdir -p $base_dir/working\n",
    "\n",
    "print(\"Restarting runtime, continue running next cells afterwards\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "base_dir = \"/content/glid-3-xl\" # base directory under which all the files related to this notebook will be saved\n",
    "model_base_dir = f\"{base_dir}/latent-diffusion/glid-3-xl/checkpoints\"\n",
    "finetune_path = f\"{model_base_dir}/finetune.pt\"\n",
    "base_path = f\"{model_base_dir}/diffusion.pt\"\n",
    "ldm_first_stage_path = f\"{model_base_dir}/kl-f8.pt\"\n",
    "inpaint_path = f\"{model_base_dir}/inpaint.pt\"\n",
    "bert_path = f\"{model_base_dir}/bert.pt\"\n",
    "\n",
    "clip_variant = 'ViT-L/14'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Some options that need to be set BEFORE pressing Run All (run > restart to change later on)\n",
    "# Models:\n",
    "'''\n",
    "\"jack\": This is the base model finetuned on a clean dataset of photographs by Jack000. It will produce better, higher resolution realistic images without watermarks,\n",
    "but might not be as good at flat illustrations, some prompts and writing text\n",
    "\n",
    "\"base\": This is the base 1.6B model released by CompVis trained on LAION-400M. It is better at illustrations but will sometimes produce blurry and watermarked images,\n",
    "write text even if unwanted and follow the prompt less.\n",
    "\n",
    "\"inpaint\": This is an inpainting model trained by jack0. If you use this, you have to set a mask image and use the Kaggle GUI.\n",
    "The mask should be the image size and black for spots to fill in, and white for areas to keep. (also try to avoid antialiasing)\n",
    "'''\n",
    "which_model = \"base\" # jack, base, inpaint\n",
    "\n",
    "# GUIs:\n",
    "'''\n",
    "Kaggle: GUI using Jupyter Forms. It will show up in the notebook and have a small progress preview if you're generating a single image, but the layout is simpler,\n",
    "there's no API or queue and you can't share it with others\n",
    "\n",
    "Gradio: [Does not support the inpaint model] GUI using Gradio. It will give you a gradio.app link (as well as embed in the notebook) with a better layout\n",
    "that you can share with others, as well as an inbuilt API, but there's no progress preview.\n",
    "'''\n",
    "which_gui = \"gradio\" # kaggle, gradio\n",
    "\n",
    "steps = 25 # How many steps diffusion should run for. Not much improvement above 25, lower values might lose detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411555ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $model_base_dir\n",
    "%cd $model_base_dir\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/bert.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/kl-f8.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/diffusion.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/finetune.pt\n",
    "!wget –quiet https://dall-3.com/models/glid-3-xl/inpaint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889ec7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd $base_dir/latent-diffusion/\n",
    "from SwinIR_wrapper.SwinIR_wrapper import SwinIR_SR\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#@title Setup Super Resolution Model { run: \"auto\" }\n",
    "pretrained_model = \"real_sr x4\" #@param [\"real_sr x4\", \"classical_sr x2\", \"classical_sr x3\", \"classical_sr x4\", \"classical_sr x8\", \"lightweight x2\", \"lightweight x3\", \"lightweight x4\"]\n",
    "\n",
    "model_type, scale = pretrained_model.split(' ')\n",
    "scale = int(scale[1])\n",
    "\n",
    "# initialize super resolution model\n",
    "sr = SwinIR_SR(model_type, scale)\n",
    "\n",
    "print(f'Loaded {pretrained_model} successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef730",
   "metadata": {},
   "source": [
    "#### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a224b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "\n",
    "sys.path.append(f\"{base_dir}/latent-diffusion/glid-3-xl\")\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from sample_api import *\n",
    "\n",
    "from dalle_pytorch import DiscreteVAE, VQGanVAE\n",
    "\n",
    "from einops import rearrange\n",
    "from math import log2, sqrt\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "from encoders.modules import BERTEmbedder\n",
    "\n",
    "import clip\n",
    "\n",
    "args = Args(which_model))\n",
    "\n",
    "model_params, models = load_models(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5eebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def swinUpscale(path, showLarger):\n",
    "    smallImg = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    hqImg = sr.upscale(smallImg)\n",
    "    # now downscale again, so it looks sharp\n",
    "    if showLarger == False:\n",
    "        resized_image = cv2.resize(hqImg, (0,0), fx=0.25, fy=0.25) \n",
    "    else:\n",
    "        resized_image = cv2.resize(hqImg, (0,0), fx=0.5, fy=0.5) \n",
    "    cv2.imwrite(path,resized_image)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de928c",
   "metadata": {},
   "source": [
    "#### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "import time\n",
    "# from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Image as PImage\n",
    "# from IPython.display import display as PDisplay\n",
    "from os.path import exists\n",
    "import shutil\n",
    "import glob\n",
    "import gradio as gr\n",
    "\n",
    "output_dir = \"./output\"\n",
    "\n",
    "%cd $base_dir/latent-diffusion/glid-3-xl\n",
    "def adv_run(prompt,negative,init_image,skips,guidance,batches,amount_per_batch,width,height,clip_rerank,swin_input,show_large):\n",
    "        args.text = prompt\n",
    "        args.negative = negative\n",
    "        if init_image != None:\n",
    "            args.init_image = init_image\n",
    "        else:\n",
    "            args.init_image = None\n",
    "        args.skip_timesteps = skips\n",
    "        args.guidance_scale = guidance\n",
    "        args.num_batches = batches\n",
    "        args.batch_size = amount_per_batch\n",
    "        args.width = width\n",
    "        args.height = height\n",
    "        args.clip_score = clip_rerank\n",
    "        shutil.rmtree(output_dir, True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        do_run(args, model_params, models)\n",
    "        print(f\"current working directory: {os.getcwd()}\")\n",
    "        if args.batch_size > 1:\n",
    "            if swin_input == True:\n",
    "                for file in tqdm(glob.glob(f\"{output_dir}/*.png\")):\n",
    "                    swinUpscale(file,show_large)\n",
    "            !montage -geometry +1+1 -background black $base_dir/output/*.png $base_dir/grid.png\n",
    "            return Image.open(f\"{base_dir}/grid.png\")\n",
    "        if swin_input == True and args.batch_size == 1:\n",
    "            swinUpscale(f\"{output_dir}/_progress_00000.png\",show_large)\n",
    "            return Image.open(f\"{output_dir}/_progress_00000.png\")\n",
    "        if swin_input == False and args.batch_size == 1:\n",
    "            return Image.open(f\"{output_dir}/_progress_00000.png\")\n",
    "\n",
    "iface = gr.Interface(fn=adv_run, inputs=[\"text\",\"text\",gr.inputs.Image(shape=(256, 256), optional=True, type=\"filepath\"),gr.inputs.Slider(0, steps,1,default=0,label=\"Step Skips (required for init image)\"),gr.inputs.Slider(1, 15,1,default=5),gr.inputs.Slider(1, 32,1,default=1),gr.inputs.Slider(1, 16,1,default=1),gr.inputs.Slider(16, 512, 16,default=256),gr.inputs.Slider(16, 512, 16,default=256), gr.inputs.Checkbox(default=False, label=\"Clip Rerank (for batch images)\", optional=False),gr.inputs.Checkbox(default=True, label=\"Increase sharpness using SwinIR\", optional=False),gr.inputs.Checkbox(default=False, label=\"Show SwinIR results as 512x512 (less sharp)\", optional=False)\n",
    "], outputs=\"image\")\n",
    "iface.launch(share=True,debug=True, inline=False, enable_queue = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
